{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTISE: TEST THE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config/config.py\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the LLM Smoke Test Framework.\"\"\"\n",
    "\n",
    "    # Crawler settings\n",
    "    HEADLESS: bool = True\n",
    "    PAGE_LOAD_TIMEOUT: int = 30\n",
    "    CAPTURE_SCREENSHOTS: bool = True\n",
    "    ANALYZE_LAYOUT: bool = True\n",
    "    CHROME_DRIVER_PATH: Optional[str] = None\n",
    "\n",
    "    # Output settings\n",
    "    OUTPUT_DIR: str = \"output\"\n",
    "    BASE_URL: str = \"\"  # Base URL for the application under test\n",
    "\n",
    "    # LLM settings\n",
    "    OPENAI_API_KEY: Optional[str] = None\n",
    "    LLM_MODEL: str = \"gpt-4o-mini\"  # Using non-vision model\n",
    "    LLM_TEMPERATURE: float = 0.0\n",
    "    LLM_MAX_TOKENS: int = 500  # Further reduced for split analysis\n",
    "    LLM_MAX_CONTEXT: int = 8000  # Maximum context size for mini model\n",
    "    VISUAL_ANALYSIS_TOKENS: int = 300  # Specific limit for visual analysis\n",
    "\n",
    "    # Screenshot optimization settings\n",
    "    SCREENSHOT_MAX_DIMENSION: int = 1280  # Maximum dimension in pixels\n",
    "    SCREENSHOT_QUALITY: int = 75  # JPEG quality (1-100)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Load configuration from environment variables.\"\"\"\n",
    "        # Load from environment variables\n",
    "        self.OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", self.OPENAI_API_KEY)\n",
    "        self.CHROME_DRIVER_PATH = os.getenv(\n",
    "            \"CHROME_DRIVER_PATH\", self.CHROME_DRIVER_PATH\n",
    "        )\n",
    "        self.OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", self.OUTPUT_DIR)\n",
    "        self.BASE_URL = os.getenv(\"BASE_URL\", self.BASE_URL)\n",
    "        self.LLM_MODEL = os.getenv(\"LLM_MODEL\", self.LLM_MODEL)\n",
    "        self.LLM_TEMPERATURE = float(\n",
    "            os.getenv(\"LLM_TEMPERATURE\", str(self.LLM_TEMPERATURE))\n",
    "        )\n",
    "        self.LLM_MAX_TOKENS = int(os.getenv(\"LLM_MAX_TOKENS\", str(self.LLM_MAX_TOKENS)))\n",
    "\n",
    "        # Create output directories\n",
    "        self._create_output_directories()\n",
    "\n",
    "        self.validate()\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate configuration.\"\"\"\n",
    "        if not self.OPENAI_API_KEY:\n",
    "            raise ValueError(\"OPENAI_API_KEY must be set\")\n",
    "\n",
    "    def _create_output_directories(self):\n",
    "        \"\"\"Create all required output directories.\"\"\"\n",
    "        directories = {\n",
    "            \"page_data_path\": os.path.join(self.OUTPUT_DIR, \"page_data\"),\n",
    "            \"analysis_path\": os.path.join(self.OUTPUT_DIR, \"analysis\"),\n",
    "            \"test_scripts_path\": os.path.join(self.OUTPUT_DIR, \"test_scripts\"),\n",
    "            \"screenshots_path\": os.path.join(self.OUTPUT_DIR, \"screenshots\"),\n",
    "        }\n",
    "\n",
    "        for path in directories.values():\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Add directory paths as properties\n",
    "        for name, path in directories.items():\n",
    "            setattr(self, name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRAWLERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM ANALYZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found the project_root. appending to the path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = \"/Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/PROFESSIONAL/proj_llm_smoke_test_framework\"\n",
    "if project_root not in sys.path:\n",
    "    print(\"found the project_root. appending to the path\")\n",
    "    sys.path.append(project_root)\n",
    "else:\n",
    "    print(\n",
    "        f\"did not find the project root. changing the working directory to project root :{project_root}\"\n",
    "    )\n",
    "    os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from config.config import Config\n",
    "\n",
    "from openai import OpenAI\n",
    "import re\n",
    "from core.screenshot_utils import optimize_screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAnalyzer:\n",
    "    \"\"\"\n",
    "    LLM-based analyzer that processes page data and generates test information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        \"\"\"\n",
    "        Initialize the LLM analyzer.\n",
    "\n",
    "        Args:\n",
    "            config (Config): Configuration object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.config.validate()  # Ensure required settings are present\n",
    "\n",
    "        # Initialize OpenAI client\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=self.config.OPENAI_API_KEY,\n",
    "            model=self.config.LLM_MODEL,\n",
    "            temperature=self.config.LLM_TEMPERATURE,\n",
    "            max_tokens=self.config.LLM_MAX_TOKENS,\n",
    "        )\n",
    "\n",
    "        # Direct OpenAI client for vision capabilities\n",
    "        self.openai_client = OpenAI(api_key=self.config.OPENAI_API_KEY)\n",
    "\n",
    "    def analyze_page(self, page_data):\n",
    "        \"\"\"Analyze page data to identify key elements for testing.\"\"\"\n",
    "        try:\n",
    "            # More aggressive data simplification\n",
    "            def truncate_text(text, max_length=500):\n",
    "                return (\n",
    "                    text[:max_length]\n",
    "                    if isinstance(text, str) and len(text) > max_length\n",
    "                    else text\n",
    "                )\n",
    "\n",
    "            # Create a simplified version of the page data to stay within token limits\n",
    "            simplified_data = {\n",
    "                \"url\": page_data.get(\"url\", \"\"),\n",
    "                \"title\": page_data.get(\"title\", \"\"),\n",
    "            }\n",
    "\n",
    "            # Process elements - only keep key interactive elements\n",
    "            if \"elements\" in page_data:\n",
    "                # Sort elements by importance (prefer elements with IDs, then with text content)\n",
    "                def element_importance(elem):\n",
    "                    has_id = elem.get(\"id\", \"\") != \"\"\n",
    "                    has_text = elem.get(\"text\", \"\") != \"\"\n",
    "                    has_name = elem.get(\"name\", \"\") != \"\"\n",
    "                    interactive = elem.get(\"tag\", \"\") in [\n",
    "                        \"button\",\n",
    "                        \"a\",\n",
    "                        \"input\",\n",
    "                        \"select\",\n",
    "                    ]\n",
    "                    return (interactive, has_id, has_text, has_name)\n",
    "\n",
    "                elements = page_data.get(\"elements\", [])\n",
    "                sorted_elements = sorted(elements, key=element_importance, reverse=True)\n",
    "\n",
    "                # Take the top N elements and simplify them\n",
    "                simplified_elements = []\n",
    "                for elem in sorted_elements[:20]:  # Increased from 10 to 20 elements\n",
    "                    simple_elem = {}\n",
    "                    # Only keep the most important attributes\n",
    "                    for key in [\"id\", \"tag\", \"type\", \"name\", \"text\", \"class\"]:\n",
    "                        if key in elem and elem[key]:\n",
    "                            # Truncate text values to reduce token count\n",
    "                            if isinstance(elem[key], str) and len(elem[key]) > 100:\n",
    "                                simple_elem[key] = elem[key][:100] + \"...\"\n",
    "                            else:\n",
    "                                simple_elem[key] = elem[key]\n",
    "                    simplified_elements.append(simple_elem)\n",
    "\n",
    "                simplified_data[\"elements\"] = simplified_elements\n",
    "\n",
    "            # Process forms - very important for testing\n",
    "            if \"forms\" in page_data:\n",
    "                forms = page_data.get(\"forms\", [])\n",
    "                simplified_forms = []\n",
    "                for form in forms[:3]:\n",
    "                    simple_form = {\n",
    "                        \"id\": form.get(\"id\", \"\"),\n",
    "                        \"action\": form.get(\"action\", \"\"),\n",
    "                        \"method\": form.get(\"method\", \"\"),\n",
    "                        \"inputs\": [],\n",
    "                    }\n",
    "                    for input_field in form.get(\"inputs\", [])[:5]:\n",
    "                        simple_input = {}\n",
    "                        for key in [\"id\", \"name\", \"type\", \"required\"]:\n",
    "                            if key in input_field and input_field[key]:\n",
    "                                simple_input[key] = input_field[key]\n",
    "                        simple_form[\"inputs\"].append(simple_input)\n",
    "                    simplified_forms.append(simple_form)\n",
    "\n",
    "                simplified_data[\"forms\"] = simplified_forms\n",
    "\n",
    "            # Process headings - helpful for understanding page structure\n",
    "            if \"headings\" in page_data:\n",
    "                headings = page_data.get(\"headings\", [])\n",
    "                simplified_headings = []\n",
    "                for heading in headings[:5]:\n",
    "                    simplified_headings.append(\n",
    "                        {\n",
    "                            \"level\": heading.get(\"level\", \"\"),\n",
    "                            \"text\": truncate_text(heading.get(\"text\", \"\"), 100),\n",
    "                        }\n",
    "                    )\n",
    "                simplified_data[\"headings\"] = simplified_headings\n",
    "\n",
    "            # Make the prompt clearer and more structured\n",
    "            prompt_template = \"\"\"\n",
    "            I'm an expert web tester analyzing a webpage to generate smoke test information.\n",
    "\n",
    "            PAGE URL: {url}\n",
    "            PAGE TITLE: {title}\n",
    "\n",
    "            ELEMENTS:\n",
    "            {elements_info}\n",
    "\n",
    "            FORMS:\n",
    "            {forms_info}\n",
    "\n",
    "            HEADINGS:\n",
    "            {headings_info}\n",
    "\n",
    "            Based on this data, I need to provide:\n",
    "\n",
    "            1. KEY ELEMENTS:\n",
    "            List the most important elements that should be tested.\n",
    "\n",
    "            2. UNIQUE IDENTIFIERS:\n",
    "            List unique ways to identify this page in tests (title, URL patterns, unique elements).\n",
    "\n",
    "            3. RECOMMENDED SMOKE TEST STEPS:\n",
    "            List 5-10 concise steps for smoke testing this page.\n",
    "\n",
    "            4. SUGGESTED LOCATOR STRATEGIES:\n",
    "            List element: locator pairs for important elements (use best practice selectors).\n",
    "            \"\"\"\n",
    "\n",
    "            # Format element info\n",
    "            elements_info = \"ELEMENTS:\\n\"\n",
    "            for elem in simplified_data.get(\"elements\", []):\n",
    "                elements_info += f\"- {elem.get('tag', '')}\"\n",
    "                if elem.get(\"id\"):\n",
    "                    elements_info += f\" id='{elem.get('id')}'\"\n",
    "                if elem.get(\"type\"):\n",
    "                    elements_info += f\" type='{elem.get('type')}'\"\n",
    "                if elem.get(\"text\"):\n",
    "                    elements_info += f\" text='{elem.get('text')}'\"\n",
    "                elements_info += \"\\n\"\n",
    "\n",
    "            # Format form info\n",
    "            forms_info = \"FORMS:\\n\"\n",
    "            for form in simplified_data.get(\"forms\", []):\n",
    "                forms_info += f\"- Form\"\n",
    "                if form.get(\"id\"):\n",
    "                    forms_info += f\" id='{form.get('id')}'\"\n",
    "                forms_info += f\" method='{form.get('method', '')}'\\n\"\n",
    "                for input_field in form.get(\"inputs\", []):\n",
    "                    forms_info += f\"  - Input\"\n",
    "                    if input_field.get(\"id\"):\n",
    "                        forms_info += f\" id='{input_field.get('id')}'\"\n",
    "                    if input_field.get(\"type\"):\n",
    "                        forms_info += f\" type='{input_field.get('type')}'\"\n",
    "                    if input_field.get(\"name\"):\n",
    "                        forms_info += f\" name='{input_field.get('name')}'\"\n",
    "                    forms_info += \"\\n\"\n",
    "\n",
    "            # Format headings info\n",
    "            headings_info = \"HEADINGS:\\n\"\n",
    "            for heading in simplified_data.get(\"headings\", []):\n",
    "                headings_info += (\n",
    "                    f\"- H{heading.get('level', '')}: {heading.get('text', '')}\\n\"\n",
    "                )\n",
    "\n",
    "            # Format the prompt\n",
    "            formatted_prompt = prompt_template.format(\n",
    "                url=simplified_data.get(\"url\", \"\"),\n",
    "                title=simplified_data.get(\"title\", \"\"),\n",
    "                elements_info=elements_info,\n",
    "                forms_info=forms_info,\n",
    "                headings_info=headings_info,\n",
    "            )\n",
    "\n",
    "            # get LLM response\n",
    "            print(len(formatted_prompt))\n",
    "            response = self.llm.invoke(formatted_prompt)\n",
    "            return self._process_analysis_response(response.content, simplified_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            # logger.error(f\"Error analyzing page with LLM: {str(e)}\")\n",
    "            return {\n",
    "                \"url\": page_data.get(\"url\", \"\"),\n",
    "                \"title\": page_data.get(\"title\", \"\"),\n",
    "                \"error\": str(e),\n",
    "                \"page_title_validation\": page_data.get(\"title\", \"\"),\n",
    "                \"unique_identifiers\": [\"URL: \" + page_data.get(\"url\", \"\")],\n",
    "                \"key_elements\": [],\n",
    "                \"smoke_test_steps\": [\n",
    "                    \"Visit the page and verify it loads\",\n",
    "                    f\"Check page title is '{page_data.get('title', '')}'\",\n",
    "                ],\n",
    "                \"locator_strategies\": {},\n",
    "            }\n",
    "\n",
    "    def _process_analysis_response(self, response_content, page_data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
